{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fce16711",
   "metadata": {},
   "source": [
    "Text classification based on recurrent neural networks. Three architectures were tested: a classic RNN, an LSTM, and a GRU. Bidirectioal variants was also tested.\n",
    "\n",
    "**1.Import libraries and parse arguments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b724985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--bidirectional'], dest='bidirectional', nargs=None, const=None, default=True, type=<class 'bool'>, choices=None, required=False, help='Use bidirectional RNN.', metavar=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import argparse\n",
    "import torch\n",
    "from typing import Tuple\n",
    "\n",
    "# Parse arguments \n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--batch_size\", default=16, type=int, help=\"Batch size.\")\n",
    "parser.add_argument(\"--epochs\", default=20, type=int, help=\"Number of epochs.\")\n",
    "parser.add_argument(\"--rnn\", default=\"GRU\", choices=[\"LSTM\", \"GRU\", \"RNN\"], help=\"RNN layer type.\")\n",
    "parser.add_argument(\"--rnn_dim\", default=64, type=int, help=\"RNN layer dimension.\")\n",
    "parser.add_argument(\"--seed\", default=42, type=int, help=\"Random seed.\")\n",
    "parser.add_argument(\"--threads\", default=1, type=int, help=\"Maximum number of threads to use.\")\n",
    "parser.add_argument(\"--we_dim\", default=128, type=int, help=\"Word embedding dimension.\")\n",
    "parser.add_argument(\"--bidirectional\", default=True, type=bool, help=\"Use bidirectional RNN.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b06b0f",
   "metadata": {},
   "source": [
    "**2.Tokenize the words and encode them using unique IDs.** Filter out words that appear only once (to reduce noise, though further testing is needed to confirm its usefulness) and limit the vocabulary to a predefined maximum size (max_size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cfbc852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Tokenize -----\n",
    "def simple_tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "def build_vocab(texts, min_freq=2, max_size=None):\n",
    "    word_counts = {}\n",
    "    for t in texts:\n",
    "        tokens = simple_tokenize(t)\n",
    "        for tok in tokens:\n",
    "            if tok in word_counts:\n",
    "                word_counts[tok] += 1\n",
    "            else:\n",
    "                word_counts[tok] = 1\n",
    "\n",
    "    # pořadí: PAD, UNK, pak nejčastější tokeny\n",
    "    vocab = {\"<pad>\":0, \"<unk>\":1}\n",
    "\n",
    "    # Filter out unique words\n",
    "    items = [(w,c) for w,c in word_counts.items() if c >= min_freq] \n",
    "\n",
    "    # sort from the most frquent\n",
    "    items.sort(key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    # Reduce size\n",
    "    if max_size is not None:\n",
    "        items = items[:max_size]\n",
    "\n",
    "    # add unique ids\n",
    "    for w,_ in items:\n",
    "        vocab[w] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def encode(X, vocab):\n",
    "    ids = []\n",
    "    for text in X:\n",
    "        ids_text = []\n",
    "        for tok in simple_tokenize(text):\n",
    "            ids_text.append(vocab.get(tok, vocab[\"<unk>\"]))\n",
    "        ids.append(ids_text)\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47fc46b",
   "metadata": {},
   "source": [
    "**3.Data and batch loaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ccc680b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X_ids, y):\n",
    "        self.X = X_ids\n",
    "        self.y = list(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "# collate function\n",
    "def collate_fn(batch, pad_id):\n",
    "    word_ids, labels = zip(*batch)\n",
    "\n",
    "    word_ids = [torch.as_tensor(x, dtype=torch.long) for x in word_ids]\n",
    "\n",
    "    x = torch.nn.utils.rnn.pad_sequence(word_ids, batch_first=True, padding_value=pad_id)\n",
    "    y = torch.as_tensor(labels, dtype=torch.long)\n",
    "    lengths = torch.as_tensor([len(xi) for xi in word_ids], dtype=torch.long)\n",
    "\n",
    "    return x, y, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341dae24",
   "metadata": {},
   "source": [
    "**4.Define model architecture.** First, word embedding layer. Second, RNN layer, and finally output linear layer with 5 outputs (corresponding to the 5 classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9a2630d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, args: argparse.Namespace, train_string_vocab) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Create all needed layers.\n",
    "        # Create a `torch.nn.Embedding` layer, embedding the word ids\n",
    "        # from `train.words.string_vocab` to dimensionality `args.we_dim`.\n",
    "        self._word_embedding = torch.nn.Embedding(\n",
    "            num_embeddings=len(train_string_vocab),\n",
    "            embedding_dim=args.we_dim,\n",
    "            padding_idx=train_string_vocab[\"<pad>\"]\n",
    "        )\n",
    "\n",
    "        # Create an RNN layer, either `torch.nn.RNN`, `torch.nn.LSTM` or `torch.nn.GRU` \n",
    "        # depending on `args.rnn`. The result should be better if the layer will be \n",
    "        # bidirectional (`bidirectional=True`) with dimensionality `args.rnn_dim`. \n",
    "        # During the model computation, the layer will process the word embeddings \n",
    "        # generated by the `self._word_embedding` layer, and we will sum the outputs \n",
    "        # of forward and backward directions.\n",
    "        if args.rnn == \"LSTM\":\n",
    "            self._word_rnn = torch.nn.LSTM(\n",
    "                input_size=args.we_dim,\n",
    "                hidden_size=args.rnn_dim,\n",
    "                bidirectional=args.bidirectional,\n",
    "                batch_first=True,\n",
    "            )\n",
    "        elif args.rnn == \"GRU\":\n",
    "            self._word_rnn = torch.nn.GRU(\n",
    "                input_size=args.we_dim,\n",
    "                hidden_size=args.rnn_dim,\n",
    "                bidirectional=args.bidirectional,\n",
    "                batch_first=True,\n",
    "            )\n",
    "        elif args.rnn == \"RNN\":\n",
    "            self._word_rnn = torch.nn.RNN(\n",
    "                input_size=args.we_dim, \n",
    "                hidden_size=args.rnn_dim, \n",
    "                bidirectional=args.bidirectional, \n",
    "                batch_first=True,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported RNN type: {args.rnn}\")\n",
    "\n",
    "        # Create an output linear layer (`torch.nn.Linear`) processing the RNN output, \n",
    "        # producing logits for tag prediction\n",
    "        self._output_layer = torch.nn.Linear(\n",
    "            in_features=args.rnn_dim,\n",
    "            out_features=5,\n",
    "        )\n",
    "\n",
    "    def forward(self, word_ids: torch.Tensor, lengths: torch.tensor) -> torch.Tensor:\n",
    "        # TODO: Start by embedding the `word_ids` using the word embedding layer.\n",
    "        emb = self._word_embedding(word_ids)\n",
    "\n",
    "        # Process the embedded words through the RNN layer. Because the sentences\n",
    "        # have different length, we have to use `torch.nn.utils.rnn.pack_padded_sequence`\n",
    "        # to construct a variable-length `PackedSequence` from the input.\n",
    "        # Finally, also pass `batch_first=True` and `enforce_sorted=False` to the call.\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        if isinstance(self._word_rnn, torch.nn.LSTM):\n",
    "            _, (h_n, _) = self._word_rnn(packed)\n",
    "        else:\n",
    "            _, h_n = self._word_rnn(packed)\n",
    "\n",
    "        if self._word_rnn.bidirectional:\n",
    "            h = h_n[-2] + h_n[-1] # sum finall hidden states from both directions\n",
    "            logits = self._output_layer(h)\n",
    "        else:\n",
    "            h = h_n[-1]\n",
    "            logits = self._output_layer(h)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d0fe0c",
   "metadata": {},
   "source": [
    "**5.Train and evaluate functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35b7ab42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_fn: torch.nn.Module,\n",
    "    device: torch.device,\n",
    ") -> Tuple[float, float]:\n",
    "    model.train()\n",
    "    total_loss, total_correct, total_examples = 0.0, 0, 0\n",
    "\n",
    "    for x, y, lengths in dataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x, lengths)\n",
    "        loss = loss_fn(logits, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = logits.argmax(dim=1)\n",
    "            total_correct += (preds == y).sum().item()\n",
    "            total_examples += y.size(0)\n",
    "            total_loss += loss.item() * y.size(0)\n",
    "\n",
    "    avg_loss = total_loss / max(1, total_examples)\n",
    "    acc = total_correct / max(1, total_examples)\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    loss_fn: torch.nn.Module,\n",
    "    device: torch.device,\n",
    ") -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total_examples = 0.0, 0, 0\n",
    "\n",
    "    for x, y, lengths in dataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "\n",
    "        logits = model(x, lengths)\n",
    "        loss = loss_fn(logits, y)\n",
    "\n",
    "        preds = logits.argmax(dim=1)\n",
    "        total_correct += (preds == y).sum().item()\n",
    "        total_examples += y.size(0)\n",
    "        total_loss += loss.item() * y.size(0)\n",
    "\n",
    "    avg_loss = total_loss / max(1, total_examples)\n",
    "    acc = total_correct / max(1, total_examples)\n",
    "    return avg_loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85e8e8b",
   "metadata": {},
   "source": [
    "**6. Main loop.** The main loop was implemented in a standard Python script and executed from the terminal. In this Jupyter Notebook version, loading arguments from the command line does not make much sense, but the original code was kept unchanged for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13ffd534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/20 | train loss 1.5885, acc 0.267 | val loss 1.5595, acc 0.299\n",
      "Epoch 02/20 | train loss 1.4421, acc 0.438 | val loss 1.4872, acc 0.373\n",
      "Epoch 03/20 | train loss 1.3015, acc 0.560 | val loss 1.4071, acc 0.467\n",
      "Epoch 04/20 | train loss 1.1357, acc 0.665 | val loss 1.3071, acc 0.515\n",
      "Epoch 05/20 | train loss 0.9225, acc 0.767 | val loss 1.1567, acc 0.566\n",
      "Epoch 06/20 | train loss 0.6442, acc 0.828 | val loss 0.9093, acc 0.658\n",
      "Epoch 07/20 | train loss 0.4003, acc 0.913 | val loss 0.8162, acc 0.701\n",
      "Epoch 08/20 | train loss 0.2419, acc 0.956 | val loss 0.6896, acc 0.744\n",
      "Epoch 09/20 | train loss 0.1640, acc 0.974 | val loss 0.5976, acc 0.787\n",
      "Epoch 10/20 | train loss 0.0982, acc 0.996 | val loss 0.5884, acc 0.804\n",
      "Epoch 11/20 | train loss 0.1063, acc 0.982 | val loss 0.6275, acc 0.778\n",
      "Epoch 12/20 | train loss 0.0615, acc 0.997 | val loss 0.5466, acc 0.811\n",
      "Epoch 13/20 | train loss 0.0339, acc 1.000 | val loss 0.5466, acc 0.827\n",
      "Epoch 14/20 | train loss 0.0297, acc 1.000 | val loss 0.5783, acc 0.822\n",
      "Epoch 15/20 | train loss 0.0221, acc 0.999 | val loss 0.5276, acc 0.836\n",
      "Epoch 16/20 | train loss 0.0154, acc 1.000 | val loss 0.5365, acc 0.838\n",
      "Epoch 17/20 | train loss 0.0125, acc 1.000 | val loss 0.5825, acc 0.829\n",
      "Epoch 18/20 | train loss 0.0101, acc 1.000 | val loss 0.5763, acc 0.834\n",
      "Epoch 19/20 | train loss 0.0083, acc 1.000 | val loss 0.5789, acc 0.838\n",
      "Epoch 20/20 | train loss 0.0077, acc 1.000 | val loss 0.5724, acc 0.854\n",
      "Best val acc: 0.854 (model saved to best_model.pt)\n"
     ]
    }
   ],
   "source": [
    "def main(args: argparse.Namespace):\n",
    "    #  Load data \n",
    "    data = pd.read_csv(\"df_file.csv\")\n",
    "    X_train, X_val, y_train, y_val = train_test_split(data[\"Text\"], data[\"Label\"], test_size=0.20, random_state=42)\n",
    "\n",
    "    # Tokenize\n",
    "    vocab = build_vocab(X_train, max_size=25000)\n",
    "    X_train_ids = encode(X_train, vocab)\n",
    "    X_val_ids = encode(X_val, vocab)\n",
    "\n",
    "    # Load datasets\n",
    "    train_ds = TextDataset(X_train_ids, y_train)\n",
    "    val_ds   = TextDataset(X_val_ids,   y_val)\n",
    "\n",
    "    pad_id = vocab[\"<pad>\"]\n",
    "    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True,\n",
    "                            collate_fn=lambda b: collate_fn(b, pad_id))\n",
    "    val_loader   = DataLoader(val_ds, batch_size=args.batch_size, shuffle=False,\n",
    "                            collate_fn=lambda b: collate_fn(b, pad_id))\n",
    "    \n",
    "    # model, optimizer, loss\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Model(args, vocab).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, loss_fn, device)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:02d}/{args.epochs} | \"\n",
    "            f\"train loss {train_loss:.4f}, acc {train_acc:.3f} | \"\n",
    "            f\"val loss {val_loss:.4f}, acc {val_acc:.3f}\"\n",
    "        )\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        print(f\"Best val acc: {best_val_acc:.3f} (model saved to best_model.pt)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_args = parser.parse_args([] if \"__file__\" not in globals() else None)\n",
    "    main(main_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223e38aa",
   "metadata": {},
   "source": [
    "Results: <br>\n",
    "epochs=20, batch=16 <br>\n",
    "\n",
    "| Model | Bidirectional | Accuracy |\n",
    "|:------|:--------------:|:---------:|\n",
    "| RNN   | False          | 0.449     |\n",
    "| RNN   | True           | 0.661     |\n",
    "| LSTM  | False          | 0.703     |\n",
    "| LSTM  | True           | 0.804     |\n",
    "| GRU   | False          | 0.724     |\n",
    "| GRU   | True           | 0.831     |\n",
    "\n",
    "To get better results: tune hyperparameters (mainly rnn_dim and we_dim), use pretrained tokenizers (e.g. Word2Vec developed by Tomáš Mikolov), \n",
    "add cosine decay, try adding more layers, or switch to transformers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PRAML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
